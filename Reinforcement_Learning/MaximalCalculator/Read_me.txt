In this folder, we are using different reinforcement learning method training an agent to deal with the same problem.

The problem:  Given an initial number, Score, and a number, N default to be 2. 
              Within the finite steps, 
	      in every step agent have to choose a calculate operator(+, -, *, /) in 
              order to maximize the Score.
              
              
              
Q learning: Using a table record every state and the corresponding q value. An agent will interact with the environment and have a feedback called reward. Then modify q table according to the reward. After steps of training, q table converge. The final q table is the action criterion of agent. The problem of Q learning is efficiency and exploration. If state space is infinite, q table will grow so large to record every states and need more epochs to let agent explore the whole space or global solution will be skipped. Thus q learning is useful when state space is finite and agent can travel the whole space easily.

Deep Q Network(DQN): Due to the disadvantage of q table, we find the neural network is something that good at record. Then DQN is the optimized method by replace q table with a neural network. This can help us deal with the recording and prediction of the new state. But the exploration problem still not fix yet. And there is another difficulty. In the algorithm, we have to predict the next and next states. Network always predict the future and never catch up the state and overfit it.

Fixed Q Deep Q Network: Another network are needed. We call it network 2. When predict future state. The original network will predict next state. And using network 2 predict the next next state. And we won't train network 2 often. After several training to the original network, clone the original network to network 2. This help prevent chasing.

Double Deep Q Network(DDQN): This is an optimized method of fixed Q method. In fixed Q method, each network predict each q value. And Double DQN using the original network predict the next state and next next state. But use the q value that generated by network 2 to improve the network.

Dueling Deep Q Network(DDQN): This method let network separate at the middle. Parallel compute the value, advantage and q value. Then merge at the end of the network. The learning efficiency and converge speed are improved under dueling.

Noisy Dense: Exploration problem is an issue in reinforcement learning. The previous way to let agent explore is add a percent value called greedy-epsilon. For example, given greedy equal to 0.9, that means it has 10 percent probability to choose action randomly and 90 percent to choose the best action predicted by network. Then raise greedy to 1 over time. Noisy Dense is a layer at the bottom of the network. When training Noisy Dense layer alway provide Gaussian noise to the output of the network. Since the noise disturbance, agent will travel around effectively.
